---
title: "tp1tdvi"
output: pdf_document
date: "2024-08-19"
---

```{r}
library(rpart)
library(rpart.plot)
library(MLmetrics)

```

```{r}
df <- read.csv("mushroom.csv")


#formateo de datos

#lógicos

df$does.bruise.or.bleed <- df$does.bruise.or.bleed == "t"
df$has.ring <- df$has.ring == "t"
df$class <- df$class == "p"

#categóricos

df$cap.shape <- as.factor(df$cap.shape)
df$cap.surface <- as.factor(df$cap.surface)
df$cap.color <- as.factor(df$cap.color)
df$gill.attachment <- as.factor(df$gill.attachment)
df$gill.spacing <- as.factor(df$gill.spacing)
df$gill.color <- as.factor(df$gill.color)
df$stem.root <- as.factor(df$stem.root)
df$stem.surface <- as.factor(df$stem.surface)
df$stem.color <- as.factor(df$stem.color)
df$veil.type <- as.factor(df$veil.type)
df$veil.color <- as.factor(df$veil.color)
df$ring.type <- as.factor(df$ring.type)
df$spore.print.color <- as.factor(df$spore.print.color)
df$habitat <- as.factor(df$habitat)
df$season <- as.factor(df$season)


head(df)


```
```{r}
#Visualización

```

```{r}
set.seed(123)  # Para reproducibilidad

# Número total de filas
total_rows <- nrow(df)

# Crear índices aleatorios
indices <- sample(1:total_rows)

# Calcular el tamaño de cada subconjunto
train_size <- floor(0.7 * total_rows)
validation_size <- floor(0.15 * total_rows)

# Dividir los índices
train_indices <- indices[1:train_size]
validation_indices <- indices[(train_size + 1):(train_size + validation_size)]
test_indices <- indices[(train_size + validation_size + 1):total_rows]

# Crear los subconjuntos
train_data <- df[train_indices, ]
validation_data <- df[validation_indices, ]
test_data <- df[test_indices, ]

y_train <- train_data$class
y_validation <- validation_data$class
y_test <- test_data$class



head(train_data)
```
```{r}
#Análisis de datos 
install.packages("ggplot2")
library(ggplot2)


# Venenosos vs No venenosos
barplot(table(df$class),
        main = "Cantidad de Hongos Venenosos y No Venenosos",
        xlab = "Clase",
        ylab = "Cantidad",
        col = c("lightcoral", "lightgreen"),
        names.arg = c("No Venenoso", "Venenoso"))

#Variables numericas
# > num -> no venenoso

boxplot(df$stem.height ~ df$class,
        main = "Box Plot de Stem Height por Clase",
        xlab = "Clase",
        ylab = "Stem Height",
        col = c("lightcoral", "lightgreen"),
        names = c("No Venenoso", "Venenoso"))

boxplot(df$stem.width ~ df$class,
        main = "Box Plot de Stem Width por Clase",
        xlab = "Clase",
        ylab = "Stem Width",
        col = c("lightcoral", "lightgreen"),
        names = c("No Venenoso", "Venenoso"))

boxplot(df$cap.diameter ~ df$class,
        main = "Box Plot de Cap Diameter por Clase",
        xlab = "Clase",
        ylab = "Cap Diameter",
        col = c("lightcoral", "lightgreen"),
        names = c("No Venenoso", "Venenoso"))
```


```{r}
#Variables categóricas
ggplot(df, aes(x = cap.color, fill = class)) +
  geom_bar(position = "stack") +
  labs(title = "Distribución de Cap Color por Clase (Venenoso/No Venenoso)",
       x = "Cap Color",
       y = "Cantidad",
       fill = "Clase") +
  scale_fill_manual(values =  c("lightcoral", "lightgreen")) +
  theme_minimal()

ggplot(df, aes(x = cap.shape, fill = class)) +
  geom_bar(position = "stack") +
  labs(title = "Distribución de Cap Color por Clase (Venenoso/No Venenoso)",
       x = "Cap Color",
       y = "Cantidad",
       fill = "Clase") +
  scale_fill_manual(values =  c("lightcoral", "lightgreen")) +
  theme_minimal()
```

```{r}
set.seed(20)

# Crear el modelo de árbol de decisión usando todas las columnas como predictores
tree <- rpart(formula = class ~ ., 
              data = train_data, 
              method = "class")

```

```{r}
# Configurar el archivo de salida con mayor resolución
png(filename = "arbol_decision.png", width = 1600, height = 1200, res = 300)

# Generar el gráfico
rpart.plot(tree)

# Cerrar el dispositivo gráfico
dev.off()


```

```{r}
# Imprimir los valores por defecto de los hiperparámetros
cat("Valores de los hiperparámetros por defecto:\n")
cat("minsplit:", tree$control$minsplit, "\n")
cat("minbucket:", tree$control$minbucket, "\n")
cat("cp:", tree$control$cp, "\n")
cat("maxdepth:", tree$control$maxdepth, "\n")
cat("xval:", tree$control$xval, "\n")
cat("maxcompete:", tree$control$maxcompete, "\n")
#teoría!!!!
#exp del arbol
```

```{r}

y_pred  <- predict(tree, newdata = test_data, type = "class")

```

```{r}
ConfusionMatrix(y_pred, y_test)
```

```{r}
Accuracy(y_pred, y_test)
```

```{r}
Precision(y_test, y_pred)
Recall(y_test, y_pred)

```
```{r}
F1_Score(y_test, y_pred)
```
```{r}
AUC(y_pred, y_test)
```
```{r}
# Definir los rangos de valores para maxdepth, minsplit, y minbucket
maxdepth_range <- 10:20  
minsplit_range <- 2:5
minbucket_range <- 1:12

# Crear una lista para almacenar las precisiones
AUC_scores <- list()
AUC_best <- 0
params_best <- list(0, 0, 0)
tree_best <- NA
cont<- 0


# Entrenar y evaluar el modelo para cada combinación de hiperparámetros
for (maxdepth in maxdepth_range) {
  for (minsplit in minsplit_range) {
    for (minbucket in minbucket_range) {
    
      # Crear y entrenar el modelo
      tree <- rpart(class ~ ., 
                     data = train_data, 
                     method = "class",
                     control = rpart.control(maxdepth = maxdepth, 
                                             minsplit = minsplit, 
                                             minbucket = minbucket,
                                             cp = 0, 
                                             xval = 0))
      
      
      y_pred <- predict(tree, newdata = validation_data, type = "class")
      AUC_curr <- AUC(y_pred, y_validation)
      
      AUC_scores[[paste("maxdepth=", maxdepth, "minsplit=", minsplit, "minbucket=", minbucket, sep="")]] <- AUC_curr
      
      if(AUC_curr > AUC_best){
        AUC_best <- AUC_curr
        params_best[1] <- maxdepth
        params_best[2] <- minsplit
        params_best[3] <- minbucket
        tree_best <- tree
      }
      print(AUC_curr)
      cont <- cont +1
      print(cont)
      
    }
  }
}

```

```{r}

#tenemos tree_best
y_pred <- predict(tree_best, newdata = test_data, type = "class")
AUC(y_pred, y_test)
#comparar este valor con el del punto 3 
```

```{r}
#7.
print(params_best)
print(AUC_best)

```

