---
title: "tp1tdvi"
output: pdf_document
date: "2024-08-19"
---

```{r}
library(rpart)
library(rpart.plot)
library(MLmetrics)
library(ggplot2)

```

```{r}
df <- read.csv("mushroom.csv")


#formateo de datos

#lógicos

df$does.bruise.or.bleed <- df$does.bruise.or.bleed == "t"
df$has.ring <- df$has.ring == "t"
df$class <- df$class == "p"

#categóricos

df$cap.shape <- as.factor(df$cap.shape)
df$cap.surface <- as.factor(df$cap.surface)
df$cap.color <- as.factor(df$cap.color)
df$gill.attachment <- as.factor(df$gill.attachment)
df$gill.spacing <- as.factor(df$gill.spacing)
df$gill.color <- as.factor(df$gill.color)
df$stem.root <- as.factor(df$stem.root)
df$stem.surface <- as.factor(df$stem.surface)
df$stem.color <- as.factor(df$stem.color)
df$veil.type <- as.factor(df$veil.type)
df$veil.color <- as.factor(df$veil.color)
df$ring.type <- as.factor(df$ring.type)
df$spore.print.color <- as.factor(df$spore.print.color)
df$habitat <- as.factor(df$habitat)
df$season <- as.factor(df$season)


head(df)


```

```{r}
set.seed(123)  # Para reproducibilidad

# Número total de filas
total_rows <- nrow(df)

# Crear índices aleatorios
indices <- sample(1:total_rows)

# Calcular el tamaño de cada subconjunto
train_size <- floor(0.7 * total_rows)
validation_size <- floor(0.15 * total_rows)

# Dividir los índices
train_indices <- indices[1:train_size]
validation_indices <- indices[(train_size + 1):(train_size + validation_size)]
test_indices <- indices[(train_size + validation_size + 1):total_rows]

# Crear los subconjuntos
train_data <- df[train_indices, ]
validation_data <- df[validation_indices, ]
test_data <- df[test_indices, ]

y_train <- train_data$class
y_validation <- validation_data$class
y_test <- test_data$class



head(train_data)
```
```{r}
#Análisis de datos

##### conteo de hongos venenosos y no venenosos
counts <- table(df$class)

barplot_obj <- barplot(counts,
                       main = "Cantidad de Hongos Venenosos y No Venenosos",
                       xlab = "Clase",
                       ylab = "Cantidad",
                       col = c("lightcoral", "lightgreen"),
                       names.arg = c("No Venenoso", "Venenoso"),
                       ylim = c(0, 30000))

# Agregar counts a las barras
text(x = barplot_obj, 
     y = counts, 
     label = as.vector(counts),  
     pos = 3,        
     cex = 0.8, 
     col = "black")

```

```{r}
#####Variables numericas
boxplot(df$stem.height ~ df$class,
        main = "Box Plot de Stem Height por Clase",
        xlab = "Clase",
        ylab = "Stem Height",
        col = c("lightcoral", "lightgreen"),
        names = c("No Venenoso", "Venenoso"))

boxplot(df$stem.width ~ df$class,
        main = "Box Plot de Stem Width por Clase",
        xlab = "Clase",
        ylab = "Stem Width",
        col = c("lightcoral", "lightgreen"),
        names = c("No Venenoso", "Venenoso"))

##### Estadísticas descriptivas
calcular_estadisticas <- function(data, columna) {
  
  stats <- aggregate(data[[columna]] ~ class, data = data, FUN = function(x) c(mean = mean(x, na.rm = TRUE), 
                                                                               median = median(x, na.rm = TRUE), 
                                                                               sd = sd(x, na.rm = TRUE)))
  stats <- do.call(data.frame, stats)
  
  quartiles_stats <- aggregate(data[[columna]] ~ class, data = data, FUN = function(x) {
    Q1 <- quantile(x, 0.25, na.rm = TRUE)
    Q3 <- quantile(x, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    return(c(Q1 = Q1, Q3 = Q3, IQR = IQR))
  })
  quartiles_stats <- do.call(data.frame, quartiles_stats)
  
  # Unir no venenosos y venenosos
  final_stats <- merge(stats, quartiles_stats, by = "class")
  
  # Renombre
  colnames(final_stats) <- c("Clase", "Media", "Mediana", "Desvio Estándar", "Q1 (25%)", "Q3 (75%)", "IQR")
  
  return(final_stats)
}


print(calcular_estadisticas(df, "stem.height"))
print(calcular_estadisticas(df, "stem.width"))


```


```{r}
#Variables categóricas

ggplot(df_counts_color, aes(x = cap.color, y = n, fill = class)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(title = "Distribución de Cap Color por Clase (Venenoso/No Venenoso)",
       x = "Cap Color",
       y = "Cantidad",
       fill = "Clase") +
  scale_fill_manual(values = c("lightcoral", "lightgreen"),
                    labels = c("No venenoso", "Venenoso")) +
  theme_minimal()

ggplot(df, aes(x = cap.shape, fill = class)) +
  geom_bar(position = "stack") +
  labs(title = "Distribución de Cap Shape por Clase (Venenoso/No Venenoso)",
       x = "Cap Color",
       y = "Cantidad",
       fill = "Clase") +
  scale_fill_manual(values = c("lightcoral", "lightgreen"),
                    labels = c("No venenoso", "Venenoso")) +
  theme_minimal() 
```

```{r}
set.seed(20)

# Creamos el modelo de árbol de decisión usando todas las columnas como predictores
tree <- rpart(formula = class ~ ., 
              data = train_data, 
              method = "class")

```

```{r}

rpart.plot(tree)

```

```{r}
# Imprimir los valores por defecto de los hiperparámetros

cat("Valores de los hiperparámetros por defecto:\n")
cat("minsplit:", tree$control$minsplit, "\n")
cat("minbucket:", tree$control$minbucket, "\n")
cat("cp:", tree$control$cp, "\n")
cat("maxdepth:", tree$control$maxdepth, "\n")
cat("xval:", tree$control$xval, "\n")

```

```{r}
y_pred  <- predict(tree, newdata = test_data, type = "class")
```

```{r}
ConfusionMatrix(y_pred, y_test)
```

```{r}
Accuracy(y_pred, y_test)
```

```{r}
Precision(y_test, y_pred)
Recall(y_test, y_pred)

```
```{r}
F1_Score(y_test, y_pred)
```
```{r}
AUC(y_pred, y_test)
```
```{r}
optimizar_hiperparametros <- function(train_data, validation_data, maxdepth_range, minsplit_range, minbucket_range){
  # Crear una lista para almacenar las precisiones
  tabla <- data.frame(
    maxdepth = integer(),
    minsplit = integer(),
    minbucket = integer(),
    AUC_score = numeric()
  )
  cont <- 0
  
  # Entrenar y evaluar el modelo para cada combinación de hiperparámetros
  for (maxdepth in maxdepth_range) {
    for (minsplit in minsplit_range) {
      for (minbucket in minbucket_range) {
        # Crear y entrenar el modelo
        tree <- rpart(class ~ ., 
                       data = train_data, 
                       method = "class",
                       control = rpart.control(maxdepth = maxdepth, 
                                               minsplit = minsplit, 
                                               minbucket = minbucket,
                                               cp = 0, 
                                               xval = 0))
        
        y_pred <- predict(tree, newdata = validation_data, type = "class")
        AUC_curr <- AUC(y_pred, y_validation)
        
        tabla <- rbind(
          tabla,
          list(maxdepth = maxdepth, minsplit = minsplit, minbucket = minbucket, AUC_score = AUC_curr))
        
        print(AUC_curr)
        cont <- cont + 1
        print(cont)
      }
    }
  }
  
  return(tabla)
}
```

```{r}
hacer_tree_best <- function(tabla, train_data){
  tabla <- tabla[order(-tabla$AUC_score), ] # Ordena de mayor a menor según AUC score
  
  # Hiperparámetros óptimos
  maxdepth <- tabla$maxdepth[1]
  minsplit <- tabla$minsplit[1]
  minbucket <- tabla$minbucket[1]

  tree_best <- rpart(class ~ ., 
                       data = train_data, 
                       method = "class",
                       control = rpart.control(maxdepth = maxdepth, 
                                               minsplit = minsplit, 
                                               minbucket = minbucket,
                                               cp = 0, 
                                               xval = 0))
  return(tree_best)
}
```

```{r}
calcular_AUC_testeo <- function(tree, test_data){
  y_pred <- predict(tree, newdata = test_data, type = "class")
  return(AUC(y_pred, y_test))
}
```


```{r}
# Rangos para los hiperparámetros
maxdepth_range <- 10:20
minsplit_range <- seq(from = 20, to = 2020, by = 100)
minbucket_range <- seq(from = 2, to = 20, by = 2)

tabla_optima <- optimizar_hiperparametros(train_data, validation_data, maxdepth_range, minsplit_range, minbucket_range)
```

```{r}
tree_best <- hacer_tree_best(tabla_optima, train_data)

calcular_AUC_testeo(tree_best, test_data)
#comparar este valor con el del punto 3
```

```{r}
#7.

generar_NA <- function(data, semilla, proporcion){
  set.seed(semilla)
  
  data_missing <- data.frame(data)
  nrows <- nrow(data)
  
  for (col in colnames(data_missing)) {
    curr_NA <- sum(is.na(data_missing[[col]]))  # cantidad de nulls que tiene la variable
    
    if (curr_NA / nrows < proporcion && col != "class"){ # si es mayor, no hace falta modificarlo. tampoco deberíamos modificar la variable a predecir
      target_NA_count <- nrows * proporcion - curr_NA # cantidad de filas a cambiar
      
      non_NA_indices <- which(!is.na(data_missing[[col]])) # indices de los datos que no son NA
      
      indices_a_reemplazar <- sample(non_NA_indices, target_NA_count)
      
      data_missing[[col]][indices_a_reemplazar] <- NA
    }
  }
  
  return(data_missing)
}
```

```{r}
data_missing_20 <- generar_NA(df, 1234, 0.2) # 20% datos faltantes

train_data_20 <- data_missing_20[train_indices, ]
validation_data_20 <- data_missing_20[validation_indices, ]
test_data_20 <- data_missing_20[test_indices, ]

data_missing_50 <- generar_NA(df, 1234, 0.5) # 50% datos faltantes

train_data_50 <- data_missing_50[train_indices, ]
validation_data_50 <- data_missing_50[validation_indices, ]
test_data_50 <- data_missing_50[test_indices, ]

data_missing_75 <- generar_NA(df, 1234, 0.75) # 75% datos faltantes

train_data_75 <- data_missing_75[train_indices, ]
validation_data_75 <- data_missing_75[validation_indices, ]
test_data_75 <- data_missing_75[test_indices, ]
```

```{r}
tabla_optima_20 <- optimizar_hiperparametros(train_data_20, validation_data_20, maxdepth_range, minsplit_range, minbucket_range)
```

```{r}
tree_best_20 <- hacer_tree_best(tabla_optima_20, train_data_20)

calcular_AUC_testeo(tree_best_20, test_data_20)
```

```{r}
tabla_optima_50 <- optimizar_hiperparametros(train_data_50, validation_data_50, maxdepth_range, minsplit_range, minbucket_range)
```

```{r}
tree_best_50 <- hacer_tree_best(tabla_optima_50, train_data_50)

calcular_AUC_testeo(tree_best_50, test_data_50)
```

```{r}
tabla_optima_75 <- optimizar_hiperparametros(train_data_75, validation_data_75, maxdepth_range, minsplit_range, minbucket_range)
```

```{r}
tree_best_75 <- hacer_tree_best(tabla_optima_75, train_data_75)

calcular_AUC_testeo(tree_best_75, test_data_75)
```